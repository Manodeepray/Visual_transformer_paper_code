{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-01T20:47:48.422458Z","iopub.status.busy":"2024-08-01T20:47:48.421826Z","iopub.status.idle":"2024-08-01T20:47:48.439140Z","shell.execute_reply":"2024-08-01T20:47:48.438153Z","shell.execute_reply.started":"2024-08-01T20:47:48.422418Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os \n","import shutil\n","import os\n","import pandas as pd\n","import numpy as np\n","import shutil\n","from shutil import copyfile\n","from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n","                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n","                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n","                                     RandomContrast, Rescaling, Resizing, Reshape)\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","import tensorflow as tf\n","import os\n","import pandas as pd\n","import numpy as np\n","import shutil\n","from shutil import copyfile\n","from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n","                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n","                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n","                                     RandomContrast, Rescaling, Resizing, Reshape)\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import os \n","import shutil\n","import tensorflow as tf\n","# Different layers\n","from tensorflow.keras.layers import MultiHeadAttention, Input, Dense, Reshape\n","from tensorflow.keras.layers import LayerNormalization, Layer\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.layers import Conv2D, Dropout\n","# For miscellaneous functions\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow import reduce_mean, float32, range, reshape\n","from tensorflow.keras import utils\n","from tensorflow.keras.metrics import TopKCategoricalAccuracy\n","from tensorflow.nn import gelu\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras.layers import RandomRotation, RandomFlip, RandomContrast\n","\n","\n","\n","\n","\n","# For tokenization\n","from tensorflow.image import extract_patches\n","import pandas as pd\n","# For math/arrays\n","import numpy as np\n","# For plotting\n","import matplotlib.pyplot as plt\n","# For profiling\n","import time\n","import os\n","import shutil\n","from shutil import copyfile"]},{"cell_type":"markdown","metadata":{},"source":["# vit\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:47:58.610634Z","iopub.status.busy":"2024-08-01T20:47:58.610256Z","iopub.status.idle":"2024-08-01T20:47:58.616252Z","shell.execute_reply":"2024-08-01T20:47:58.615324Z","shell.execute_reply.started":"2024-08-01T20:47:58.610602Z"},"trusted":true},"outputs":[],"source":["#config\n","\n","CONFIGURATION = {\n","    \"BATCH_SIZE\": 32,\n","    \"IM_SIZE\": 121,\n","    \"LEARNING_RATE\": 1e-3,\n","    \"N_EPOCHS\": 20,\n","    \"DROPOUT_RATE\": 0.0,\n","    \"REGULARIZATION_RATE\": 0.0,\n","    \"N_FILTERS\": 6,\n","    \"KERNEL_SIZE\": 3,\n","    \"N_STRIDES\": 1,\n","    \"POOL_SIZE\": 2,\n","    \"N_DENSE_1\": 1024,\n","    \"N_DENSE_2\": 128,\n","    \"NUM_CLASSES\": 2,\n","    \"PATCH_SIZE\": 11,\n","    \"PROJ_DIM\": 363,\n","    \"CLASS_NAMES\": ['Benign','Malignant'],\n","}\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:47:59.890901Z","iopub.status.busy":"2024-08-01T20:47:59.890540Z","iopub.status.idle":"2024-08-01T20:47:59.915022Z","shell.execute_reply":"2024-08-01T20:47:59.914109Z","shell.execute_reply.started":"2024-08-01T20:47:59.890871Z"},"trusted":true},"outputs":[],"source":["\n","class PatchEncoder(Layer):\n","    def __init__(self, N_PATCHES, HIDDEN_SIZE):\n","        super(PatchEncoder, self).__init__(name='patch_encoder')\n","        self.linear_projection = Dense(HIDDEN_SIZE)\n","        self.positional_embedding = Embedding(N_PATCHES, HIDDEN_SIZE)\n","        self.N_PATCHES = N_PATCHES\n","\n","    def call(self, x):\n","        # Resize input images to (256, 256, 3)\n","        x = tf.image.resize(x, [121, 121])\n","        #x = tf.image.resize(x, [CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]])\n","        \n","        patches = tf.image.extract_patches(\n","            images=x,\n","            sizes=[1, CONFIGURATION[\"PATCH_SIZE\"], CONFIGURATION[\"PATCH_SIZE\"], 1],\n","            strides=[1, CONFIGURATION[\"PATCH_SIZE\"], CONFIGURATION[\"PATCH_SIZE\"], 1],\n","            rates=[1, 1, 1, 1],\n","            padding='VALID')\n","\n","        patch_dim = patches.shape[-1]\n","        patches = tf.reshape(patches, (tf.shape(x)[0], -1, patch_dim))\n","\n","        embedding_input = tf.range(start=0, limit=self.N_PATCHES, delta=1)\n","        output = self.linear_projection(patches) + self.positional_embedding(embedding_input)\n","\n","        return output\n","\n","    \n","    \n","class TokenLearningLayer(Layer):\n","    def __init__(self, n_maps, token_dims):\n","        super(TokenLearningLayer, self).__init__(name='Token_Learning_Layer')\n","        self.n_maps = n_maps\n","        self.token_dims = token_dims\n","    def build(self, input_shape):\n","        self.input_embed_dim = input_shape[-1]\n","        self.model = Sequential()\n","        self.model.add(Reshape((self.token_dims[0],\n","        self.token_dims[1], self.input_embed_dim)))\n","        conv2D_layer = Conv2D(filters=self.n_maps, kernel_size=(3, 3),\n","                              activation=gelu, padding=\"same\",\n","                              use_bias=False)\n","        self.model.add(conv2D_layer)\n","        conv2D_sigmoid_layer = Conv2D(filters=self.n_maps, kernel_size=(3, 3),\n","                                      activation=\"sigmoid\", padding=\"same\",\n","                                      use_bias=False)\n","        self.model.add(conv2D_sigmoid_layer)\n","        self.model.add(Reshape((-1, self.n_maps)))\n","        \n","    def call(self, inputs):\n","        total_tokens = inputs.shape[1]\n","        attn_wts = self.model(inputs)\n","        inputs = Reshape((total_tokens, 1, self.input_embed_dim))(inputs)\n","        attn_wts = Reshape((total_tokens, self.n_maps, 1))(attn_wts)\n","        attended_output = inputs*attn_wts\n","        output = reduce_mean(attended_output, axis=1)\n","        return output\n","    \n","\n","class TransformerEncoder(Layer):\n","    def __init__(self, N_HEADS, HIDDEN_SIZE):\n","        super(TransformerEncoder, self).__init__(name='transformer_encoder')\n","        self.layer_norm_1 = LayerNormalization()\n","        self.layer_norm_2 = LayerNormalization()\n","        self.multi_head_att = MultiHeadAttention(num_heads=N_HEADS, key_dim=HIDDEN_SIZE)\n","        self.dense_1 = Dense(HIDDEN_SIZE, activation=tf.nn.gelu)\n","        self.dense_2 = Dense(HIDDEN_SIZE, activation=tf.nn.gelu)\n","\n","    def call(self, inputs):\n","        x_1 = self.layer_norm_1(inputs)\n","        x_1 = self.multi_head_att(x_1, x_1)\n","        x_1 = Add()([x_1, inputs])\n","        x_2 = self.layer_norm_2(x_1)\n","        x_2 = self.dense_1(x_2)\n","        output = self.dense_2(x_2)\n","        output = Add()([output, x_1])\n","        return output\n","\n","class ViT(Model):\n","    def __init__(self, N_HEADS, HIDDEN_SIZE, N_PATCHES, N_LAYERS, N_DENSE_UNITS):\n","        super(ViT, self).__init__(name='vision_transformer')\n","        self.N_LAYERS = N_LAYERS\n","        self.patch_encoder = PatchEncoder(N_PATCHES, HIDDEN_SIZE)\n","        self.trans_encoders_1 = [TransformerEncoder(N_HEADS, HIDDEN_SIZE) for _ in range(N_LAYERS)]\n","        self.trans_encoders_2 = [TransformerEncoder(N_HEADS, HIDDEN_SIZE) for _ in range(N_LAYERS)]\n","\n","        self.dense_1 = Dense(N_DENSE_UNITS, activation=tf.nn.gelu)\n","        self.dense_2 = Dense(N_DENSE_UNITS, activation=tf.nn.gelu)\n","        self.TokenLearner = TokenLearningLayer(n_maps = 4,token_dims= (CONFIGURATION[\"PATCH_SIZE\"] , CONFIGURATION[\"PATCH_SIZE\"] , ) )\n","\n","        self.dense_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation='softmax')\n","\n","    def call(self, inputs, training=None):\n","        x = self.patch_encoder(inputs)\n","        for i in range(self.N_LAYERS):\n","            x = self.trans_encoders_1[i](x)\n","        x = self.TokenLearner(x)\n","        for i in range(self.N_LAYERS):\n","            x = self.trans_encoders_2[i](x)\n","        \n","        \n","        \n","        x = Flatten()(x)\n","        \n","        \n","        x = self.dense_1(x)\n","        x = self.dense_2(x)\n","        return self.dense_3(x)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:47:59.916915Z","iopub.status.busy":"2024-08-01T20:47:59.916645Z","iopub.status.idle":"2024-08-01T20:47:59.964152Z","shell.execute_reply":"2024-08-01T20:47:59.963473Z","shell.execute_reply.started":"2024-08-01T20:47:59.916891Z"},"trusted":true},"outputs":[],"source":["vit = ViT(\n","    N_HEADS = 4, HIDDEN_SIZE = 363, N_PATCHES = 121,\n","    N_LAYERS = 2, N_DENSE_UNITS = 128)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:48:00.442112Z","iopub.status.busy":"2024-08-01T20:48:00.441806Z","iopub.status.idle":"2024-08-01T20:48:01.487890Z","shell.execute_reply":"2024-08-01T20:48:01.486942Z","shell.execute_reply.started":"2024-08-01T20:48:00.442087Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n","array([[0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344],\n","       [0.6208165 , 0.37918344]], dtype=float32)>"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["vit(tf.zeros([32,121,121,3]))"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:48:02.474379Z","iopub.status.busy":"2024-08-01T20:48:02.474021Z","iopub.status.idle":"2024-08-01T20:48:02.501176Z","shell.execute_reply":"2024-08-01T20:48:02.500347Z","shell.execute_reply.started":"2024-08-01T20:48:02.474349Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vision_transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"vision_transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ patch_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEncoder</span>)    │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">176,055</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,378,739</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,378,739</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,378,739</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,378,739</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">185,984</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ Token_Learning_Layer            │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,212</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenLearningLayer</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ patch_encoder (\u001b[38;5;33mPatchEncoder\u001b[0m)    │ ?                      │       \u001b[38;5;34m176,055\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     \u001b[38;5;34m2,378,739\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     \u001b[38;5;34m2,378,739\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     \u001b[38;5;34m2,378,739\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder             │ ?                      │     \u001b[38;5;34m2,378,739\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │       \u001b[38;5;34m185,984\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │        \u001b[38;5;34m16,512\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ Token_Learning_Layer            │ ?                      │        \u001b[38;5;34m13,212\u001b[0m │\n","│ (\u001b[38;5;33mTokenLearningLayer\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m2\u001b[0m)                │           \u001b[38;5;34m258\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,906,977</span> (37.79 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,906,977\u001b[0m (37.79 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,906,977</span> (37.79 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,906,977\u001b[0m (37.79 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["vit.summary()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:48:09.371764Z","iopub.status.busy":"2024-08-01T20:48:09.371037Z","iopub.status.idle":"2024-08-01T20:48:09.385988Z","shell.execute_reply":"2024-08-01T20:48:09.384787Z","shell.execute_reply.started":"2024-08-01T20:48:09.371731Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n","from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy, Precision, Recall\n","\n","\n","loss_function = CategoricalCrossentropy()\n","\n","metrics = [\n","    CategoricalAccuracy(name=\"accuracy\"),\n","    TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\"),\n","    Precision(name=\"precision\"),\n","    Recall(name=\"recall\")\n","]"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:48:10.539113Z","iopub.status.busy":"2024-08-01T20:48:10.538714Z","iopub.status.idle":"2024-08-01T20:48:10.551074Z","shell.execute_reply":"2024-08-01T20:48:10.549840Z","shell.execute_reply.started":"2024-08-01T20:48:10.539079Z"},"trusted":true},"outputs":[],"source":["vit.compile(optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"] ),\n","           loss = loss_function,\n","           metrics = metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-01T20:48:11.590249Z","iopub.status.busy":"2024-08-01T20:48:11.589372Z"},"trusted":true},"outputs":[],"source":["train_directory = \"/kaggle/input/for-isic-2024-paper/val_test_train_dataset/test_train_dataset/train\"\n","test_directory = \"/kaggle/input/for-isic-2024-paper/val_test_train_dataset/test_train_dataset/test\"\n","\n","val_directory = \"/kaggle/input/for-isic-2024-paper/val_test_train_dataset/test_train_dataset/val\"\n","\n","\n","train_dataset = tf.keras.utils.image_dataset_from_directory(\n","    train_directory,\n","    labels='inferred',\n","    label_mode='categorical',\n","    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n","    color_mode='rgb',\n","    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n","    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n","    shuffle=True,\n","    seed=99,\n",")\n","test_dataset = tf.keras.utils.image_dataset_from_directory(\n","    test_directory,\n","    labels='inferred',\n","    label_mode='categorical',\n","    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n","    color_mode='rgb',\n","    batch_size=1,#CONFIGURATION[\"BATCH_SIZE\"],\n","    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n","    shuffle=True,\n","    seed=99,\n",")\n","\n","\n","val_dataset = tf.keras.utils.image_dataset_from_directory(\n","    val_directory,\n","    labels='inferred',\n","    label_mode='categorical',\n","    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n","    color_mode='rgb',\n","    batch_size=1,#CONFIGURATION[\"BATCH_SIZE\"],\n","    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n","    shuffle=True,\n","    seed=99,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["augment_layers = tf.keras.Sequential([\n","  RandomRotation(factor = (-0.025, 0.025)),\n","  RandomFlip(mode='horizontal',),\n","  RandomContrast(factor=0.1),\n","])\n","def augment_layer(image, label):\n","    return augment_layers(image, training = True), label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_dataset = (\n","    train_dataset\n","    .map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","testing_dataset = (\n","    test_dataset\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","validation_dataset = (\n","    val_dataset\n","    .prefetch(tf.data.AUTOTUNE)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","\n","start_time = time.time()\n","\n","history = vit.fit(\n","    training_dataset,\n","    validation_data = validation_dataset,\n","    epochs = CONFIGURATION[\"N_EPOCHS\"],\n","verbose = 1)\n","\n","end_time = time.time()\n","\n","# Calculate the time taken\n","time_taken = end_time - start_time\n","\n","# Print the time taken in a readable format\n","print(f\"Time taken for training: {time_taken:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainin time = time_taken"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy vs. Validation Accuracy')\n","plt.legend()\n","\n","# Plot loss vs val_loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss vs. Validation Loss')\n","plt.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import confusion_matrix, f1_score\n","\n","predictions = model.predict(test_dataset)\n","predicted_classes = np.argmax(predictions, axis=1)\n","\n","true_labels = np.concatenate([y for x, y in test_dataset], axis=0)\n","true_classes = np.argmax(true_labels, axis=1)\n","\n","conf_matrix = confusion_matrix(true_classes, predicted_classes)\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","f1 = f1_score(true_classes, predicted_classes, average='weighted')\n","print(\"F1 Score: {:.4f}\".format(f1))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluation_results = model.evaluate(testing_dataset)\n","for metric_name, metric_value in zip(model.metrics_names, evaluation_results):\n","    print(f\"{metric_name}: {metric_value:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5479730,"sourceId":9082368,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
